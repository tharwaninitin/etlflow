// When the user clicks on the search box, we want to toggle the search dropdown
function displayToggleSearch(e) {
  e.preventDefault();
  e.stopPropagation();

  closeDropdownSearch(e);
  
  if (idx === null) {
    console.log("Building search index...");
    prepareIdxAndDocMap();
    console.log("Search index built.");
  }
  const dropdown = document.querySelector("#search-dropdown-content");
  if (dropdown) {
    if (!dropdown.classList.contains("show")) {
      dropdown.classList.add("show");
    }
    document.addEventListener("click", closeDropdownSearch);
    document.addEventListener("keydown", searchOnKeyDown);
    document.addEventListener("keyup", searchOnKeyUp);
  }
}

//We want to prepare the index only after clicking the search bar
var idx = null
const docMap = new Map()

function prepareIdxAndDocMap() {
  const docs = [  
    {
      "title": "BigQuery",
      "url": "/etlflow/site/docs/bigquery.html",
      "content": "Google Cloud BigQuery Steps This page shows different BigQuery Steps available in this library BQLoadStep Below are the Input/Output formats supported by this step: Input Formats =&gt; CSV, JSON, ORC, PARQUET, BQ Output Formats =&gt; BQ Example 1 (Source=&gt;CSV on GCS, Destination=&gt;BQ) Here input is CSV file on GCS location with schema specified by case class Rating and output is BigQuery table. import etlflow.etlsteps.BQLoadStep import etlflow.spark.IOType import etlflow.gcp.BQInputType sealed trait MyEtlJobSchema case class Rating(user_id: Int, movie_id: Int, rating: Double, timestamp: Long) extends MyEtlJobSchema val step1 = BQLoadStep[Rating]( name = \"LoadRatingBQ\", input_location = Left(\"gs://path/to/input/*\"), input_type = BQInputType.CSV(), output_dataset = \"test\", output_table = \"ratings\" ) // step1: BQLoadStep[Rating] = etlflow.etlsteps.BQLoadStep@584ab978 Example 2 (Source=&gt;BQ, Destination=&gt;BQ) Here input is BigQuery SQL and output is BigQuery table. import etlflow.etlsteps.BQLoadStep import com.google.cloud.bigquery.{FormatOptions, JobInfo} import etlflow.spark.IOType import etlflow.gcp.BQInputType val select_query: String = \"\"\" | SELECT movie_id, COUNT(1) cnt | FROM test.ratings | GROUP BY movie_id | ORDER BY cnt DESC; |\"\"\".stripMargin // select_query: String = \"\"\" // SELECT movie_id, COUNT(1) cnt // FROM test.ratings // GROUP BY movie_id // ORDER BY cnt DESC; // \"\"\" val step2 = BQLoadStep( name = \"LoadQueryDataBQ\", input_location = Left(select_query), input_type = BQInputType.BQ, output_dataset = \"test\", output_table = \"ratings_grouped\", output_create_disposition = JobInfo.CreateDisposition.CREATE_IF_NEEDED ) // step2: BQLoadStep[Nothing] = etlflow.etlsteps.BQLoadStep@2553d95c Example 3 (Source=&gt;Seq(BQ), Destination=&gt;BQ) Here input is sequence of BigQuery SQL with date partition and output is again BigQuery table. Using this step we can load multiple different partitions in parallel in BigQuery. import etlflow.etlsteps.BQLoadStep import com.google.cloud.bigquery.{FormatOptions, JobInfo} import etlflow.spark.IOType import etlflow.gcp.BQInputType val getQuery: String =&gt; String = param =&gt; s\"\"\" | SELECT date, movie_id, COUNT(1) cnt | FROM test.ratings_par | WHERE date = '$param' | GROUP BY date, movie_id | ORDER BY cnt DESC; |\"\"\".stripMargin // getQuery: String =&gt; String = &lt;function1&gt; val input_query_partitions = Seq( (getQuery(\"2016-01-01\"),\"20160101\"), (getQuery(\"2016-01-02\"),\"20160102\") ) // input_query_partitions: Seq[(String, String)] = List( // ( // \"\"\" // SELECT date, movie_id, COUNT(1) cnt // FROM test.ratings_par // WHERE date = '2016-01-01' // GROUP BY date, movie_id // ORDER BY cnt DESC; // \"\"\", // \"20160101\" // ), // ( // \"\"\" // SELECT date, movie_id, COUNT(1) cnt // FROM test.ratings_par // WHERE date = '2016-01-02' // GROUP BY date, movie_id // ORDER BY cnt DESC; // \"\"\", // \"20160102\" // ) // ) val step3 = BQLoadStep( name = \"LoadQueryDataBQPar\", input_location = Right(input_query_partitions), input_type = BQInputType.BQ, output_dataset = \"test\", output_table = \"ratings_grouped_par\" ) // step3: BQLoadStep[Nothing] = etlflow.etlsteps.BQLoadStep@4251d8b BQQueryStep We can use below step when we want to just run some query/stored-procedure on BigQuery without returning anything. import etlflow.etlsteps.BQQueryStep import com.google.cloud.bigquery.{FormatOptions, JobInfo} import etlflow.spark.IOType import etlflow.gcp.BQInputType val step4 = BQQueryStep( name = \"CreateTableBQ\", query = s\"\"\"CREATE OR REPLACE TABLE test.ratings_grouped as SELECT movie_id, COUNT(1) cnt FROM test.ratings GROUP BY movie_id ORDER BY cnt DESC;\"\"\".stripMargin ) // step4: BQQueryStep = etlflow.etlsteps.BQQueryStep@526b0cff"
    } ,    
    {
      "title": "Etlflow Cloud",
      "url": "/etlflow/site/docs/cloud.html",
      "content": "Etlflow Cloud Etlflow Cloud is one of component of etlflow library which we can use to define the steps related to GCP dataproc or AWS cluster like s3 sensor step, gcp put step, gcp dataproc cluster creation/deletion step etc. Etlflow cloud library contains sub-components like : AWS : This module can get used when we want to read/write data to and from aws buckets. GCP : This module can get used when we want to read/write data to and from gcp buckets. To use etlflow cloud library in project add below setting in build.sbt file : lazy val docs = (project in file(\"modules/examples\")) .settings( scalaVersion := \"2.12.13\", libraryDependencies ++= List(\"com.github.tharwaninitin\" %% \"etlflow-core\" % \"0.10.0\")) STEP 1) Building an assembly jar for etlflow cloud To build an assembly jar for an etlflow cloud library we need to perform some steps. Clone this git repo and go inside repo root folder and perform below steps: &gt; sbt &gt; project cloud &gt; assembly Inside root folder run ‘sbt’ command Inside sbt, Run ‘project cloud’ to set the current project as etlflow-cloud Once project gets set then run ‘assembly’ to build an assembly jar. STEP 2) Copy the etlflow cloud assembly jar to gcs bucket using below command gsutil cp modules/cloud/target/scala-2.12/etlflow-cloud-assembly-x.x.x.jar gs://&lt;BUCKET-NAME&gt;/jars/examples Replace x.x.x with the latest version Example : Below is the step of cloud library using which we can run the query on hive cluster : import etlflow.etlsteps.{DPHiveJobStep, DPSparkJobStep} import etlflow.schema.Executor.DATAPROC import etlflow.etljobs.GenericEtlJob import etlflow.EtlJobProps case class EtlJob1Props ( inpuut_query: String = \"SELECT 1 AS ONE\" ) extends EtlJobProps case class EtlJob1(job_properties: EtlJob1Props) extends GenericEtlJob[EtlJob1Props] { val dpConfig = DATAPROC( sys.env(\"DP_PROJECT_ID\"), sys.env(\"DP_REGION\"), sys.env(\"DP_ENDPOINT\"), sys.env(\"DP_CLUSTER_NAME\") ) val step = DPHiveJobStep( name = \"DPHiveJobStepExample\", query = \"SELECT 1 AS ONE\", config = dpConfig, ) val job = for { _ &lt;- step.execute() } yield () }"
    } ,    
    {
      "title": "Cloud Steps",
      "url": "/etlflow/site/docs/cloud_steps.html",
      "content": "Cloud Steps This page shows different Cloud Steps available in this library CloudStoreSyncStep : local to gcs We can use below step when we want to store data from local storage to gcs bucket. import etlflow.etlsteps.CloudStoreSyncStep import etlflow.utils.Location val gcs_output_location = \"&lt;GCS PATH&gt;\" // gcs_output_location: String = \"&lt;GCS PATH&gt;\" val gcs_bucket = \"&lt;GCS BUCKET&gt;\" // gcs_bucket: String = \"&lt;GCS BUCKET&gt;\" val step = CloudStoreSyncStep( name = \"GCStoLOCALStep\" , input_location = Location.LOCAL(\"modules/core/src/test/resources/input/movies/ratings/\") , output_location = Location.GCS(gcs_bucket,gcs_output_location) , output_overwrite = true , parallelism = 3 , chunk_size = 1000 * 1024 ) // step: CloudStoreSyncStep = CloudStoreSyncStep( // \"GCStoLOCALStep\", // LOCAL(\"modules/core/src/test/resources/input/movies/ratings/\", \"localhost\"), // GCS(\"&lt;GCS BUCKET&gt;\", \"&lt;GCS PATH&gt;\", None), // None, // true, // 3, // 1024000 // ) CloudStoreSyncStep : local to aws We can use below step when we want to store data from local storage to aws bucket. import etlflow.etlsteps.CloudStoreSyncStep import etlflow.utils.Location import software.amazon.awssdk.regions.Region val s3_bucket = \"&lt;AWS BUCKET&gt;\" // s3_bucket: String = \"&lt;AWS BUCKET&gt;\" val s3_input_location = \"&lt;AWS PATH&gt;\" // s3_input_location: String = \"&lt;AWS PATH&gt;\" lazy val s3_region: Region = Region.AP_SOUTH_1 val step1 = CloudStoreSyncStep( name = \"LOCALtoS3Step\" , input_location = Location.LOCAL(\"modules/core/src/test/resources/input/movies/ratings/\") , output_location = Location.S3(s3_bucket, s3_input_location, s3_region) , output_overwrite = true , chunk_size = 1000 * 1024 ) // step1: CloudStoreSyncStep = CloudStoreSyncStep( // \"LOCALtoS3Step\", // LOCAL(\"modules/core/src/test/resources/input/movies/ratings/\", \"localhost\"), // S3(\"&lt;AWS BUCKET&gt;\", \"&lt;AWS PATH&gt;\", ap-south-1, None), // None, // true, // 1, // 1024000 // ) CloudStoreSyncStep : aws to local We can use below step when we want to store data from aws bucket to local storage. import etlflow.etlsteps.CloudStoreSyncStep import etlflow.utils.Location val step2 = CloudStoreSyncStep( name = \"S3toLOCALStep\" , input_location = Location.S3(s3_bucket, s3_input_location, s3_region) , output_location = Location.LOCAL(\"modules/core/src/test/resources/s3_output/\") , output_overwrite = true ) // step2: CloudStoreSyncStep = CloudStoreSyncStep( // \"S3toLOCALStep\", // S3(\"&lt;AWS BUCKET&gt;\", \"&lt;AWS PATH&gt;\", ap-south-1, None), // LOCAL(\"modules/core/src/test/resources/s3_output/\", \"localhost\"), // None, // true, // 1, // 32768 // ) CloudStoreSyncStep : gcs to local We can use below step when we want to store data from gcs bucket to local storage. import etlflow.etlsteps.CloudStoreSyncStep import etlflow.utils.Location val gcs_input_location = \"&lt;GCS PATH&gt;\" // gcs_input_location: String = \"&lt;GCS PATH&gt;\" val step3 = CloudStoreSyncStep( name = \"GCStoLOCALStep\" , input_location = Location.GCS(gcs_bucket, gcs_input_location) , output_location = Location.LOCAL(\"modules/core/src/test/resources/gcs_output/\") , output_overwrite = true , parallelism = 3 ) // step3: CloudStoreSyncStep = CloudStoreSyncStep( // \"GCStoLOCALStep\", // GCS(\"&lt;GCS BUCKET&gt;\", \"&lt;GCS PATH&gt;\", None), // LOCAL(\"modules/core/src/test/resources/gcs_output/\", \"localhost\"), // None, // true, // 3, // 32768 // )"
    } ,    
    {
      "title": "Components",
      "url": "/etlflow/site/docs/components.html",
      "content": "Etlflow Components Etlflow library has four components : Core Spark Cloud Server"
    } ,    
    {
      "title": "Etlflow Core",
      "url": "/etlflow/site/docs/core.html",
      "content": "Quickstart (Etlflow Core) STEP 1) Define build.sbt: To use etlflow core library in project add below setting in build.sbt file lazy val root = (project in file(\".\")) .settings( scalaVersion := \"2.12.13\", libraryDependencies ++= List(\"com.github.tharwaninitin\" %% \"etlflow-core\" % \"x.x.x\")) STEP 2) Define job properties EtlJobProps.scala: Here we can have any kind of logic for creating static or dynamic input parameters for job. For e.g. intermediate path can be dynamically generated for every run based on current date. import etlflow.EtlJobProps import java.text.SimpleDateFormat import java.time.LocalDate lazy val canonical_path = new java.io.File(\".\").getCanonicalPath lazy val input_file_path = s\"$canonical_path/modules/core/src/test/resources/input/movies/ratings_parquet/ratings.parquet\" val date_prefix = LocalDate.now.toString.replace(\"-\",\"\") // date_prefix: String = \"20210721\" case class EtlJob1Props ( ratings_input_path: String = input_file_path, ratings_intermediate_bucket: String = sys.env(\"GCS_BUCKET\"), ratings_intermediate_file_key: String = s\"temp/$date_prefix/ratings.parquet\", ratings_output_dataset: String = \"test\", ratings_output_table_name: String = \"ratings\", ) extends EtlJobProps STEP 3) Define job EtlJob1.scala: import etlflow.etlsteps.{DPHiveJobStep, DPSparkJobStep} import etlflow.schema.Executor.DATAPROC import etlflow.EtlJobProps import etlflow.etljobs.GenericEtlJob import etlflow.etlsteps.DBQueryStep import etlflow.utils._ import etlflow.schema.Credential.JDBC case class EtlJob1(job_properties: EtlJob1Props) extends GenericEtlJob[EtlJob1Props] { val step = DBQueryStep( name = \"UpdatePG\", query = \"BEGIN; DELETE FROM ratings_par WHERE 1 = 1; COMMIT;\", credentials = JDBC(\"jdbc_url\", \"jdbc_user\", \"jdbc_pwd\", \"jdbc_driver\") ) val job = for { _ &lt;- step.execute() } yield () } STEP 4) Define job and properties mapping using EtlJobPropsMapping MyEtlJobPropsMapping.scala: Below is the example of defining EtlJobPropsMapping where we can define the mapping between job and its properties. import etlflow.{EtlJobPropsMapping, EtlJobProps} import etlflow.etljobs.EtlJob sealed trait MyEtlJobPropsMapping[EJP &lt;: EtlJobProps, EJ &lt;: EtlJob[EJP]] extends EtlJobPropsMapping[EJP,EJ] object MyEtlJobPropsMapping { case object Job1 extends MyEtlJobPropsMapping[EtlJob1Props,EtlJob1] { override def getActualProperties(job_properties: Map[String, String]): EtlJob1Props = EtlJob1Props() } } STEP 5) Define Main Runnable App LoadData.scala: import etlflow.{EtlFlowApp, EtlJobProps} object LoadData extends EtlFlowApp[MyEtlJobPropsMapping[EtlJobProps,EtlJob[EtlJobProps]]] STEP 6) Running jobs: To be able to use this core library, first you need postgres instance up and running, then create new database in pg (for e.g. etlflow), then set below environment variables. export LOG_DB_URL=jdbc:postgresql://localhost:5432/etlflow export LOG_DB_USER=&lt;...&gt; export LOG_DB_PWD=&lt;..&gt; export LOG_DB_DRIVER=org.postgresql.Driver Now to create database tables used in this library run below commands from repo root folder: sbt runMain LoadData run_db_migration Now to run sample job use below command: sbt runMain LoadData run_job --job_name Job1"
    } ,    
    {
      "title": "Dataproc Executor",
      "url": "/etlflow/site/docs/dataproc.html",
      "content": "Dataproc Executor When we provides the job execution as dataproc then job will gets submit on the gcs dataproc cluster with provided configurations Parameters args [EtlJobArgs] - Job Details. transactor [HikariTransactor[Task]] - Database connection details. config [DATAPROC] - DATAPROC connection details. Example 1 Below is the example to tell library about job should get executed on dataproc cluster. import etlflow.schema.Executor.DATAPROC import etlflow.schema.Executor val dataproc = DATAPROC(\"project-name\",\"region\",\"endpoint\",\"cluster-name\") // dataproc: DATAPROC = DATAPROC( // \"project-name\", // \"region\", // \"endpoint\", // \"cluster-name\", // List() // ) case class EtlJob5Props ( ratings_input_path: List[String] = List(\"\"), job_schedule: String = \"0 0 5,6 ? * *\", ratings_output_table: String = \"\", job_deploy_mode: Executor = dataproc ) In above case class we define job_deploy_mode as dataproc which tells library on how this particular jobs will gets executed."
    } ,    
    {
      "title": "Dataproc",
      "url": "/etlflow/site/docs/dp.html",
      "content": "Dataproc Steps This page shows different Dataproc Steps available in this library DPHiveJobStep We can use below step when we want to trigger query on Hive Dataproc. Query should not return results for this step import etlflow.schema.Executor.DATAPROC import etlflow.etlsteps.{DPHiveJobStep, DPSparkJobStep} val dpConfig1 = DATAPROC( \"DP_PROJECT_ID\", \"DP_REGION\", \"DP_ENDPOINT\", \"DP_CLUSTER_NAME\" ) // dpConfig1: DATAPROC = DATAPROC( // \"DP_PROJECT_ID\", // \"DP_REGION\", // \"DP_ENDPOINT\", // \"DP_CLUSTER_NAME\", // List() // ) val step1 = DPHiveJobStep( name = \"DPHiveJobStepExample\", query = \"SELECT 1 AS ONE\", config = dpConfig1, ) // step1: DPHiveJobStep = DPHiveJobStep( // \"DPHiveJobStepExample\", // \"SELECT 1 AS ONE\", // DATAPROC( // \"DP_PROJECT_ID\", // \"DP_REGION\", // \"DP_ENDPOINT\", // \"DP_CLUSTER_NAME\", // List() // ) // ) DPSparkJobStep We can use below step when we want to trigger a job on Dataproc cluster from local server. import etlflow.schema.Executor.DATAPROC import etlflow.etlsteps.{DPHiveJobStep, DPSparkJobStep} val libs = \"DP_LIBS\".split(\",\").toList // libs: List[String] = List(\"DP_LIBS\") val step2 = DPSparkJobStep( name = \"DPSparkJobStepExample\", job_name = \"DP_JOB_NAME\", props = Map.empty, config = dpConfig1, main_class = \"DP_MAIN_CLASS\", libs = libs ) // step2: DPSparkJobStep = DPSparkJobStep( // \"DPSparkJobStepExample\", // \"DP_JOB_NAME\", // Map(), // DATAPROC( // \"DP_PROJECT_ID\", // \"DP_REGION\", // \"DP_ENDPOINT\", // \"DP_CLUSTER_NAME\", // List() // ), // \"DP_MAIN_CLASS\", // List(\"DP_LIBS\") // ) DPCreateStep We can use below step when we want to create new dataproc cluster. import etlflow.etlsteps.DPCreateStep import etlflow.gcp.DataprocProperties import etlflow.schema.Executor.DATAPROC val dpConfig2 = DATAPROC( \"DP_PROJECT_ID\", \"DP_REGION\", \"DP_ENDPOINT\", \"DP_CLUSTER_NAME\" ) // dpConfig2: DATAPROC = DATAPROC( // \"DP_PROJECT_ID\", // \"DP_REGION\", // \"DP_ENDPOINT\", // \"DP_CLUSTER_NAME\", // List() // ) val dpProps = DataprocProperties( bucket_name = \"DP_BUCKET_NAME\", subnet_uri = Some(\"DP_SUBNET_WORK_URI\"), network_tags = \"DP_NETWORK_TAG1,DP_NETWORK_TAG2\".split(\",\").toList, service_account = Some(\"DP_SERVICE_ACCOUNT\") ) // dpProps: DataprocProperties = DataprocProperties( // \"DP_BUCKET_NAME\", // Some(\"DP_SUBNET_WORK_URI\"), // List(\"DP_NETWORK_TAG1\", \"DP_NETWORK_TAG2\"), // Some(\"DP_SERVICE_ACCOUNT\"), // Some(1800L), // \"n1-standard-4\", // \"n1-standard-4\", // \"1.5.4-debian10\", // \"pd-ssd\", // 400, // 200, // 1, // 3 // ) val step3 = DPCreateStep( name = \"DPCreateStepExample\", config = dpConfig2, props = dpProps ) // step3: DPCreateStep = etlflow.etlsteps.DPCreateStep@548208df DPDeleteStep We can use below step when we want to delete dataproc cluster. import etlflow.schema.Executor.DATAPROC import etlflow.etlsteps.DPDeleteStep val dpConfig3 = DATAPROC( \"DP_PROJECT_ID\", \"DP_REGION\", \"DP_ENDPOINT\", \"DP_CLUSTER_NAME\" ) // dpConfig3: DATAPROC = DATAPROC( // \"DP_PROJECT_ID\", // \"DP_REGION\", // \"DP_ENDPOINT\", // \"DP_CLUSTER_NAME\", // List() // ) val step = DPDeleteStep( name = \"DPDeleteStepExample\", config = dpConfig3, ) // step: DPDeleteStep = etlflow.etlsteps.DPDeleteStep@2d657ecc"
    } ,    
    {
      "title": "How to use Etlflow library",
      "url": "/etlflow/site/docs/example.html",
      "content": "How to use Etlflow library Clone this git repo and go inside repo root folder and enter below command (make sure you have sbt and scala installed) SBT_OPTS=\"-Xms512M -Xmx1024M -Xss2M -XX:MaxMetaspaceSize=1024M\" sbt -v \"project examples\" console Import core packages import etlflow.etlsteps._ import etlflow.utils._ import etlflow.spark.IOType import etlflow.gcp.BQInputType Define Job input and ouput locations lazy val canonical_path: String = new java.io.File(\".\").getCanonicalPath lazy val job_properties: Map[String,String] = Map( \"ratings_input_path\" -&gt; s\"$canonical_path/examples/src/main/data/movies/ratings/*\", \"ratings_output_path\" -&gt; s\"$canonical_path/examples/src/main/data/movies/output/ratings\", \"ratings_output_file_name\" -&gt; \"ratings.orc\" ) Create spark session import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory import ch.qos.logback.classic.{Level, Logger} LoggerFactory.getLogger(\"org\").asInstanceOf[Logger].setLevel(Level.WARN) LoggerFactory.getLogger(\"io\").asInstanceOf[Logger].setLevel(Level.INFO) implicit lazy val spark: SparkSession = SparkSession.builder().config(\"spark.driver.bindAddress\", \"127.0.0.1\").master(\"local[*]\").getOrCreate() Define ETL Step which will load ratings data with below schema as specified from CSV to ORC import org.apache.spark.sql.SaveMode import etlflow.etlsteps.SparkReadWriteStep import etlflow.spark.IOType import etlflow.gcp.BQInputType case class Rating(user_id:Int, movie_id: Int, rating : Double, timestamp: Long) lazy val step1 = SparkReadWriteStep[Rating]( name = \"ConvertRatingsCSVtoORC\", input_location = Seq(job_properties(\"ratings_input_path\")), input_type = IOType.CSV(), output_location = job_properties(\"ratings_output_path\"), output_type = IOType.ORC, output_save_mode = SaveMode.Overwrite, output_repartitioning_num = 1, output_repartitioning = true, output_filename = Some(job_properties(\"ratings_output_file_name\")) ) Since all these steps return Task from ZIO library, we need to import Zio Default Runtime to run these steps import zio.{Runtime,ZEnv} Run this step as below val task = step1.process() // task: zio.package.Task[Unit] = zio.ZIO$EffectPartial@3bf34f13 Now executing above step has added data in ORC format in path defined in above properties upon completion. This is very basic example for flat load from CSV to ORC but lets say you need to transform csv data in some way for e.g. new column need to be added then we need to create function with below signature: import org.apache.spark.sql.{Encoders, Dataset} import org.apache.spark.sql.types.DateType import org.apache.spark.sql.functions._ import etlflow.spark.IOType import etlflow.gcp.BQInputType case class RatingOutput(user_id:Int, movie_id: Int, rating: Double, timestamp: Long, date: java.sql.Date) def enrichRatingData(spark: SparkSession, in : Dataset[Rating]): Dataset[RatingOutput] = { val mapping = Encoders.product[RatingOutput] val ratings_df = in .withColumn(\"date\", from_unixtime(col(\"timestamp\"), \"yyyy-MM-dd\").cast(DateType)) ratings_df.as[RatingOutput](mapping) } Now our step would change to something like this lazy val step2 = SparkReadTransformWriteStep[Rating, RatingOutput]( name = \"ConvertRatingsCSVtoORC\", input_location = Seq(job_properties(\"ratings_input_path\")), input_type = IOType.CSV(), transform_function = enrichRatingData, output_type = IOType.ORC, output_save_mode = SaveMode.Overwrite, output_location = job_properties(\"ratings_output_path\"), output_repartitioning_num = 1, output_repartitioning = true, output_filename = Some(job_properties(\"ratings_output_file_name\")) ) val task1 = step1.process() // task1: zio.package.Task[Unit] = zio.ZIO$EffectPartial@6018d38c Lets add another step which will copy this transformed ORC data in BigQuery table. For this step to work correctly Google Cloud SDK needs to be installed and configured, as in this library upload from local file to BigQuery uses bq command which is only recommended to be used in testing environments as in production files should be present on Google Cloud Storage when uploading to BigQuery import etlflow.spark.IOType import etlflow.gcp.BQInputType // Adding two new properties for Bigquery table and Dataset lazy val job_properties1: Map[String,String] = Map( \"ratings_input_path\" -&gt; s\"$canonical_path/examples/src/main/data/movies/ratings/*\", \"ratings_output_path\" -&gt; s\"$canonical_path/examples/src/main/data/movies/output/ratings\", \"ratings_output_file_name\" -&gt; \"ratings.orc\", \"ratings_output_dataset\" -&gt; \"test\", \"ratings_output_table_name\" -&gt; \"ratings\" ) lazy val step3 = BQLoadStep( name = \"LoadRatingBQ\", input_location = Left(job_properties1(\"ratings_output_path\") + \"/\" + job_properties1(\"ratings_output_file_name\")), input_type = BQInputType.ORC, output_dataset = job_properties1(\"ratings_output_dataset\"), output_table = job_properties1(\"ratings_output_table_name\") ) val task2 = step2.process() // task2: zio.package.Task[Unit] = zio.ZIO$EffectPartial@34f1dfaf Now we can run also chain multiple steps together and run as single job as shown below. val job = for { _ &lt;- step1.process() _ &lt;- step2.process() } yield () // job: zio.ZIO[Any, Throwable, Unit] = zio.ZIO$FlatMap@36d227d8"
    } ,    
    {
      "title": "Executors",
      "url": "/etlflow/site/docs/executors.html",
      "content": "Executors Following is the list of executors available in this library: Dataproc Kubernetes Local Local Sub-Process"
    } ,    
    {
      "title": "Using with GCP Dataproc",
      "url": "/etlflow/site/docs/gcpdp.html",
      "content": "Using Etlflow core library in your ETL projects on GCP Dataproc Prerequisite Your system should have Google Cloud Sdk installed. You should have Dataproc cluster created. You should appropriate permissions for GCS, BigQuery, Submitting jobs on Dataproc etc STEP 1) Building a jar for application, here we will take examples scala project in this library To build jar for an application perform below commands from project root folder: &gt; sbt &gt; project examples &gt; package From root folder run ‘sbt’ command Inside sbt, Run ‘project examples’ to set the current project as examples. Once project gets set then run ‘package’ to build an assembly jar. STEP 2) Building an assembly jar for etlflow core To build an assembly jar for an etlflow core library we need to perform some steps. Clone this git repo and go inside repo root folder and perform below steps: &gt; sbt &gt; project core &gt; assembly Inside root folder run ‘sbt’ command Inside sbt, Run ‘project core’ to set the current project as etlflow-core Once project gets set then run ‘assembly’ to build an assembly jar. STEP 3) Copy the etlflow core assembly jar to gcs bucket using below command gsutil cp modules/core/target/scala-2.12/etlflow-core-assembly-x.x.x.jar gs://&lt;BUCKET-NAME&gt;/jars/examples Replace x.x.x with the latest version Note Sometime spark applications may fail if spark application dependencies conflict with Dataproc Hadoop’s dependencies. This conflict can arise because Hadoop injects its dependencies into the application’s classpath, so its dependencies take precedence over the application’s dependencies. When a conflict occurs, NoSuchMethodError or other errors can be generated. One such library is Guava, which is the Google core library for Java that is used by many libraries and frameworks, including Hadoop. A dependency conflict can occur if a job or its dependencies require a version of Guava that is newer than the one used by Hadoop. To resolve such errors we can define below settings in build.sbt for assembly jar assemblyMergeStrategy in assembly := { case PathList(\"META-INF\", xs @ _*) =&gt; MergeStrategy.discard case x =&gt; MergeStrategy.first }, assemblyShadeRules in assembly := Seq( ShadeRule.rename(\"com.google.common.**\" -&gt; \"repackaged.com.google.common.@1\").inAll ), For reference: manage-spark-dependencies STEP 4) Copy the examples jar to gcs bucket using below command: gsutil cp examples/target/scala-2.12/etlflow-examples_2.12-x.x.x.jar gs://&lt;BUCKET-NAME&gt;/jars/examples Replace x.x.x with the latest version STEP 5) Provide below detials in file examples/src/main/resources/application.conf: dbLog = { url = &lt;log_db_url&gt;, user = &lt;log_db_user&gt;, password = &lt;log_db_pwd&gt;, driver = \"org.postgresql.Driver\" } slack = { url = &lt;slack-url&gt;, env = &lt;running-env&gt; } Now Copy updated application.conf file at gcs bucket using below command: gsutil cp examples/src/main/resources/application.conf gs://&lt;BUCKET-NAME&gt;/jars/examples Copy the input file present at location examples/src/main/data/movies/ratings_parquet/ratings.parquet for running sample job using below command: gsutil cp examples/src/main/data/movies/ratings_parquet/ratings.parquet gs://&lt;BUCKET-NAME&gt;/examples/input STEP 6) Now finally we can run sample job on Dataproc using below command: gcloud dataproc jobs submit spark \\ --project &lt;project-name&gt; \\ --region &lt;region-name&gt; \\ --cluster &lt;cluster-name&gt; \\ --class examples.LoadData \\ --jars gs://&lt;BUCKET-NAME&gt;/jars/examples/etlflow-examples_2.12-0.7.19.jar,gs://&lt;BUCKET-NAME&gt;/jars/examples/etlflow-core-assembly-0.7.19.jar \\ -- run_job --job_name EtlJob1PARQUETtoORCtoBQLocalWith2Steps --props ratings_input_path=gs://&lt;BUCKET-NAME&gt;/examples/input,ratings_output_table_name=ratings,ratings_output_dataset=test,ratings_output_file_name=ratings.orc"
    } ,    
    {
      "title": "GCS Steps",
      "url": "/etlflow/site/docs/gcs.html",
      "content": "Google Cloud Storage Steps GCSPutStep This step uploads file from local filesystem to GCS bucket in specified key. Parameters bucket [String] - Name of the GCS bucket. prefix [String] - Prefix of the object. key [String] - The key(folder) inside GCS bucket. file [String] - This is the full local path of the file which needs to be uploaded. import etlflow.etlsteps.GCSPutStep val step = GCSPutStep( name = \"LoadRatingGCS\", bucket = \"gcs_bucket\", key = \"temp/ratings.parquet\", file = \"local_file_path\" ) // step: GCSPutStep = etlflow.etlsteps.GCSPutStep@771f2a77 GCSDeleteStep [Documentation coming soon]"
    } ,    
    {
      "title": "GCS Sensor",
      "url": "/etlflow/site/docs/gcssensor.html",
      "content": "Google Cloud Storage Sensor Step This step runs a GCS lookup for specified key in bucket repeatedly until it is found or fail after defined number of retries. Parameters bucket [String] - Name of the GCS bucket. prefix [String] - Prefix of the object. key [String] - The key being waited on. It is relative path from root level after prefix. retry [Int] - Number of retries before failing the step. spaced [Duration] - Each retry is done after waiting of duration specified in this field. Example 1 Below example waits for a file temp/ratings.parquet to be present in a GCS bucket named gcs_bucket. import etlflow.etlsteps.GCSSensorStep import scala.concurrent.duration._ val step = GCSSensorStep( name = \"GCSKeySensor\", bucket = \"gcs_bucket\", prefix = \"temp\", key = \"ratings.parquet\", retry = 10, spaced = 5.second ) // step: GCSSensorStep = etlflow.etlsteps.GCSSensorStep@4080ee88"
    } ,    
    {
      "title": "Gke Server Installation",
      "url": "/etlflow/site/docs/gke.html",
      "content": "Using Etlflow Server library on gke environment with deployment yaml files. Prerequisite Your system should have Kubernetes Engine installed. You should have GCP service account access with appropriate permissions for GCS, BigQuery, Submitting and deploying jobs etc. STEP 1) Building docker image for examples project. Update the file examples/src/main/resources/application.conf with below content : db-log = { url = &lt;log_db_url&gt;, user = &lt;log_db_user&gt;, password = &lt;log_db_pwd&gt;, driver = \"org.postgresql.Driver\" } slack = { url = &lt;slack-url&gt;, env = &lt;running-env&gt; } Copy GCP service account json file at location examples/src/main/conf with name cred.json To build docker image we have to run below commands. &gt; sbt &gt; project examples &gt; docker:publishLocal STEP 2) Now we have successfully built an image. Let’s deploy this image on google kubernetes engine. GKE contains the 2 types of yaml files : deployment.yaml: Provide a configuration to run the docker image. apiVersion: apps/v1 kind: Deployment metadata: name: etlflow-gke namespace: &lt;NAME-SPAECE&gt; spec: replicas: 1 selector: matchLabels: app: etlflow-gke template: metadata: labels: app: etlflow-gke spec: containers: - name: etlflow-gke # Replace $GCLOUD_PROJECT with your project ID image: gcr.io/&lt;GCLOUD_PROJECT&gt;/etlflow:0.1.0 # This app listens on port 8080 for web traffic by default. ports: - containerPort: 8080 env: - name: PORT value: \"8080\" service.yaml: Provide a configuration to access the deployed application from front-end. apiVersion: v1 kind: Service metadata: name: etlflow-gke spec: type: LoadBalancer selector: app: etlflow-gke ports: - port: 80 targetPort: 8080 Use below commands to run/stop deployed the etlflow on GKE using above two configuration files : kubectl apply -f deployment.yaml ( Deploy the resource to the cluster ) kubectl get deployments ( Track the status of the Deployment ) kubectl get pods ( To check the pods that the Deployment created) kubectl apply -f service.yaml ( To Create the service ) kubectl get services ( Get the Service's external IP address ) To setup a kubernetes cluster"
    } ,    
    {
      "title": "Using with Hadoop Cluster",
      "url": "/etlflow/site/docs/hdfs.html",
      "content": "Using Etlflow core library in your ETL projects on Hadoop Cluster. Prerequisite Your system should have one of the below distribution installed machine : Cloudera. Hortonworks. MapR You should have Hadoop cluster created. You should appropriate permissions for GCS, BigQuery, Submitting jobs on Hadoop etc STEP 1) Building a jar for application, here we will take examples scala project in this library To build jar for an application perform below commands from project root folder: &gt; sbt &gt; project examples &gt; package From root folder run ‘sbt’ command Inside sbt, Run ‘project examples’ to set the current project as examples. Once project gets set then run ‘package’ to build an assembly jar. STEP 2) Building an assembly jar for etlflow core To build an assembly jar for an etlflow core library we need to perform some steps. Clone this git repo and go inside repo root folder and perform below steps: &gt; sbt &gt; project core &gt; assembly Inside root folder run ‘sbt’ command Inside sbt, Run ‘project core’ to set the current project as etlflow-core Once project gets set then run ‘assembly’ to build an assembly jar. STEP 3) Copy the etlflow core assembly jar to hdfs location using below command hdfs dfs -copyFromLocal modules/core/target/scala-2.12/etlflow-core-assembly-x.x.x.jar hdfs://&lt;HDFS_destination&gt;/jars/etlflow Replace x.x.x with the latest version STEP 4) Copy the examples jar to hdfs location using below command: hdfs dfs -copyFromLocal examples/target/scala-2.12/etlflow-examples_2.12-x.x.x.jar hdfs://&lt;HDFS_destination&gt;/jars/etlflow Replace x.x.x with the latest version STEP 5) Provide below detials in file examples/src/main/resources/application.conf: dbLog = { url = &lt;log_db_url&gt;, user = &lt;log_db_user&gt;, password = &lt;log_db_pwd&gt;, driver = \"org.postgresql.Driver\" } slack = { url = &lt;slack-url&gt;, env = &lt;running-env&gt; } Now Copy updated application.conf file at hdfs location using below command: hdfs dfs -copyFromLocal examples/src/main/resources/application.conf hdfs://&lt;HDFS_destination&gt;/jars/etlflow Copy the input file present at location examples/src/main/data/movies/ratings_parquet/ratings.parquet for running sample job using below command: hdfs dfs -copyFromLocal examples/src/main/data/movies/ratings_parquet/ratings.parquet hdfs://&lt;HDFS_destination&gt;/jars/etlflow STEP 6) Now finally we can run sample job on Hadoop cluster using below command: spark-submit \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 8g \\ --executor-memory 16g \\ --class examples.LoadData \\ hdfs://&lt;HDFS_destination&gt;/jars/examples/etlflow-examples_2.12-0.7.19.jar,hdfs://&lt;HDFS_destination&gt;/jars/examples/etlflow-core-assembly-0.7.19.jar \\ run_job --job_name EtlJob1PARQUETtoORCtoBQLocalWith2Steps --props ratings_input_path=hdfs://&lt;HDFS_destination&gt;/examples/input,ratings_output_table_name=ratings,ratings_output_dataset=test,ratings_output_file_name=ratings.orc"
    } ,    
    {
      "title": "Core Installation",
      "url": "/etlflow/site/docs/http.html",
      "content": "Http Step This page shows different Http Step (Post/Get) available in this library Below are the supported steps : HttpStep =&gt; When we want to just callback the url. No response return HttpResponseStep =&gt; When we want to http response from the step Parameters name [String] - Description of the Step. url [String] - Specify valid URL . http_method [HttpMethod] - Http method to be used (GET/POST). params [Either(String, Seq((String,String)))] - Provide params if want to send some contents in get or post request(Optional). headers [Map[String,String]] - Provide header if any (Optional). log_response [Boolean] - This param will print the logs when provideded value is true. Default is false. Example 1 Below is the sample Http Step GET example. import etlflow.etlsteps._ import etlflow.utils.HttpMethod val step1 = HttpRequestStep[Unit]( name = \"HttpGetSimple\", url = \"https://httpbin.org/get\", method = HttpMethod.GET, log = true, connection_timeout = 1200000 ) // step1: HttpRequestStep[Unit] = HttpRequestStep( // \"HttpGetSimple\", // \"https://httpbin.org/get\", // GET, // Right(Map()), // Map(), // true, // 1200000, // 150000, // false // ) Example 2 We can use below http GET step when we want return response from the step. import etlflow.etlsteps._ import etlflow.utils.HttpMethod val step2 = HttpRequestStep[String]( name = \"HttpGetParams\", url = \"https://httpbin.org/get\", method = HttpMethod.GET, params = Right(Map(\"param1\"-&gt; \"value1\")), log = true, ) // step2: HttpRequestStep[String] = HttpRequestStep( // \"HttpGetParams\", // \"https://httpbin.org/get\", // GET, // Right(Map(\"param1\" -&gt; \"value1\")), // Map(), // true, // 10000, // 150000, // false // ) Example 3 Below is the simple Http POST json example. import etlflow.etlsteps._ import etlflow.utils.HttpMethod val step3 = HttpRequestStep[Unit]( name = \"HttpPostJson\", url = \"https://httpbin.org/post\", method = HttpMethod.POST, params = Left(\"\"\"{\"key\":\"value\"}\"\"\"), headers = Map(\"X-Auth-Token\"-&gt;\"abcd.xxx.123\"), log = true, ) // step3: HttpRequestStep[Unit] = HttpRequestStep( // \"HttpPostJson\", // \"https://httpbin.org/post\", // POST, // Left(\"{\\\"key\\\":\\\"value\\\"}\"), // Map(\"X-Auth-Token\" -&gt; \"abcd.xxx.123\"), // true, // 10000, // 150000, // false // ) Example 4 We can use below http POST step when we want return response from the step. import etlflow.etlsteps._ import etlflow.utils.HttpMethod val step4 = HttpRequestStep[String]( name = \"HttpPostForm\", url = \"https://httpbin.org/post?signup=yes\", method = HttpMethod.POST, params = Right(Map(\"name\" -&gt; \"John\", \"surname\" -&gt; \"doe\")), log = true, ) // step4: HttpRequestStep[String] = HttpRequestStep( // \"HttpPostForm\", // \"https://httpbin.org/post?signup=yes\", // POST, // Right(Map(\"name\" -&gt; \"John\", \"surname\" -&gt; \"doe\")), // Map(), // true, // 10000, // 150000, // false // )"
    } ,    
    {
      "title": "Introduction",
      "url": "/etlflow/site/docs/",
      "content": "Introduction This library provides plug-and-play steps for Apache Spark, No SQL Databases(Redis, BigQuery etc), Relational Databases(Postgres, Mysql etc), Cloud Storage(S3, GCS etc) and multiple different libraries/datasources that makes it easier to develop ETL applications in Scala which can be easily Tested and Composed together. These steps are ready to handle various tasks on Google Cloud Platform(GCP) and Amazon Web Services(AWS). Etlflow Core Features Job and Step Api This library provides job and step api using which we can write very complex ETL pipelines. InBuilt Logging Provides database and slack logging for detailed logging of each step inside job. Single Source Code All logic for entire job can be written in Scala including defining cron schedule, job properties and job definition. Many plug-and-play steps Provides many steps that are ready to handle your task on Google Cloud Platform(GCP) and Amazon Web Services(AWS). Simple Custom Step API New custom steps can be added using very simple API. Etlflow Server Features InBuilt Rest API Provides feature rich GraphQL api which is tightly integrated with core library, no need to install it separately as plugin. Jobs can be triggered externally for e.g. from GCP Cloud functions using this api. InBuilt Web-Server Server component in this libarary Provides basic UI like airflow which includes both scheduler and webserver, providing all basic functionality that airflow is providing. Sample job using Etlflow library Sensor Step: Using Sensor Step we can lookup for specified file in a bucket repeatedly until it is found or fail after defined number of retries. For example, We can use S3SensorStep, GCSSensorStep. These steps require the information on Input bucket, Output Bucket, Retry parameters etc. Data Transfer Step: Using Data Transfer Steps we can transfer the data from AWS to GCS. For Data transfer step we can use CloudStoreSyncStep. Using above mentioned steps we can transfer data from GCS-to-LOCAL, LOCAL-to-S3, S3-to-LOCAL, GCS-to-LOCAL etc. Spark Step: Using Spark Step we can load transformed data into destination(BQ,JDBC,GCS). For example, we can use SparkReadWriteStep, SparkReadTransformWriteStep. This steps can load the input bucket data, transform the same data and write into destination bucket. We can load the data from (AWS,GCS,JDBC,BQ) and we can write the data into (GCS,JDBC,BQ) using spark Step. Success/Failure Step: Using HTTP / EMAIL step we can send the success or failure notifications/emails to other teams."
    } ,    
    {
      "title": "DBQuery",
      "url": "/etlflow/site/docs/jdbc.html",
      "content": "JDBC Database Steps This page shows different JDBC Database Steps available in this library DBQueryStep We can use below step when we want to trigger query/stored-procedure on JDBC database. Query should not return results for this step import etlflow.etlsteps.DBQueryStep import etlflow.schema.Credential.JDBC val step1 = DBQueryStep( name = \"UpdatePG\", query = \"BEGIN; DELETE FROM ratings WHERE 1 =1; INSERT INTO ratings SELECT * FROM ratings_temp; COMMIT;\", credentials = JDBC(\"jdbc_url\", \"jdbc_user\", \"jdbc_pwd\", \"org.postgresql.Driver\") ) // step1: DBQueryStep = etlflow.etlsteps.DBQueryStep@f021274 DBReadStep We can use below step when we want to trigger query/stored-procedure on JDBC database. Query should return results for this step import etlflow.etlsteps.DBReadStep import etlflow.etljobs.GenericEtlJob import etlflow.schema.Credential.JDBC case class EtlJobRun(job_name: String,job_run_id: String, state: String) private def step2(cred: JDBC) = DBReadStep[EtlJobRun]( name = \"FetchEtlJobRun\", query = \"SELECT job_name,job_run_id,state FROM jobrun\", credentials = cred )"
    } ,    
    {
      "title": "Job",
      "url": "/etlflow/site/docs/job.html",
      "content": "Job Job is collection of steps. Any case class can be converted to EtlJob just by extending GenericEtlJob trait. This traits requires to implement three objects as shown below. override val job_properties: EtlJobProps = ??? override val globalProperties: Config = ??? override val job: Task[Unit] = ??? Set these environment variables export GOOGLE_APPLICATION_CREDENTIALS=&lt;...&gt; # This should be full path to GCP Service Account Key Json which should have GCS and BigQuery Read/Write access export GCS_BUCKET=&lt;...&gt; # This is the intermediate GCS bucket where data will be uploaded for this example Clone this git repo and go inside repo root folder and enter below command (make sure you have sbt and scala installed) SBT_OPTS=\"-Xms512M -Xmx1024M -Xss2M -XX:MaxMetaspaceSize=1024M\" sbt -v \"project examples\" console Create EtlJobProps Here we can have any kind of logic for creating static or dynamic input parameters for job. For e.g. intermediate path can be dynamically generated for every run based on current date. import etlflow.EtlJobProps import java.text.SimpleDateFormat import java.time.LocalDate lazy val canonical_path = new java.io.File(\".\").getCanonicalPath lazy val input_file_path = s\"$canonical_path/modules/core/src/test/resources/input/movies/ratings_parquet/ratings.parquet\" val date_prefix = LocalDate.now.toString.replace(\"-\",\"\") // date_prefix: String = \"20210721\" case class EtlJob1Props ( ratings_input_path: String = input_file_path, ratings_intermediate_bucket: String = sys.env(\"GCS_BUCKET\"), ratings_intermediate_file_key: String = s\"temp/$date_prefix/ratings.parquet\", ratings_output_dataset: String = \"test\", ratings_output_table_name: String = \"ratings\", ) extends EtlJobProps GenericEtlJob Below is the example of GenericEtlJob which has two steps which can execute in any order defined by composing ZIO effects. import com.google.cloud.bigquery.JobInfo import etlflow.etljobs.GenericEtlJob import etlflow.etlsteps.{BQLoadStep, GCSPutStep} import zio.Task import etlflow.spark.IOType import etlflow.gcp.BQInputType case class RatingOutput(user_id: Int, movie_id: Int, rating : Double, timestamp: Long, date: java.sql.Date) case class EtlJob1(job_properties: EtlJob1Props) extends GenericEtlJob[EtlJob1Props] { val step1 = GCSPutStep( name = \"LoadRatingGCS\", bucket = job_properties.ratings_intermediate_bucket, key = job_properties.ratings_intermediate_file_key, file = job_properties.ratings_input_path ) val step2 = BQLoadStep( name = \"LoadRatingBQ\", input_location = Left(s\"gs://${job_properties.ratings_intermediate_bucket}/${job_properties.ratings_intermediate_file_key}\"), input_type = BQInputType.PARQUET, output_dataset = job_properties.ratings_output_dataset, output_table = job_properties.ratings_output_table_name, output_create_disposition = JobInfo.CreateDisposition.CREATE_IF_NEEDED ) val job = for { _ &lt;- step1.execute() _ &lt;- step2.execute() } yield () } SequentialEtlJob Below is the example of SequentialEtlJob which is much simpler way to run jobs when all steps are needed to be run sequentially. import etlflow.EtlStepList import etlflow.etljobs.SequentialEtlJob import etlflow.etlsteps._ import etlflow.etlsteps.{BQLoadStep, GCSPutStep} import com.google.cloud.bigquery.JobInfo import etlflow.spark.IOType import etlflow.gcp.BQInputType case class EtlJob2(job_properties: EtlJob1Props) extends SequentialEtlJob[EtlJob1Props] { val step3 = GCSPutStep( name = \"LoadRatingGCS\", bucket = job_properties.ratings_intermediate_bucket, key = job_properties.ratings_intermediate_file_key, file = job_properties.ratings_input_path ) val step4 = BQLoadStep( name = \"LoadRatingBQ\", input_location = Left(s\"gs://${job_properties.ratings_intermediate_bucket}/${job_properties.ratings_intermediate_file_key}\"), input_type = BQInputType.PARQUET, output_dataset = job_properties.ratings_output_dataset, output_table = job_properties.ratings_output_table_name, output_create_disposition = JobInfo.CreateDisposition.CREATE_IF_NEEDED ) val etlStepList = EtlStepList(step3,step4) }"
    } ,    
    {
      "title": "Kubernates Executor",
      "url": "/etlflow/site/docs/kubernetes.html",
      "content": "Kubernetes Executor When we provides the job execution as Kubernates then job will get’s submitted on the Kubernates cluster with provided configurations Parameters args [EtlJobArgs] - Job Details. transactor [HikariTransactor[Task]] - Database connection details. config [KUBERNETES] - KUBERNETES connection details. Example 1 Below is the example to tell library about job should get executed on dataproc cluster. import etlflow.schema.Executor.KUBERNETES import etlflow.schema.Executor val kubernetes = KUBERNETES( \"etlflow:0.7.19\", \"default\", Map( \"GOOGLE_APPLICATION_CREDENTIALS\"-&gt; Option(\"&lt;cred_file&gt;\"), \"LOG_DB_URL\"-&gt; Option(\"jdbc:postgresql://host.docker.internal:5432/postgres\"), \"LOG_DB_USER\"-&gt; Option(\"&lt;username&gt;\"), \"LOG_DB_PWD\"-&gt; Option(\"&lt;pwd&gt;\"), \"LOG_DB_DRIVER\"-&gt; Option(\"org.postgresql.Driver\") ) ) // kubernetes: KUBERNETES = KUBERNETES( // \"etlflow:0.7.19\", // \"default\", // Map( // \"GOOGLE_APPLICATION_CREDENTIALS\" -&gt; Some(\"&lt;cred_file&gt;\"), // \"LOG_DB_DRIVER\" -&gt; Some(\"org.postgresql.Driver\"), // \"LOG_DB_USER\" -&gt; Some(\"&lt;username&gt;\"), // \"LOG_DB_URL\" -&gt; Some(\"jdbc:postgresql://host.docker.internal:5432/postgres\"), // \"LOG_DB_PWD\" -&gt; Some(\"&lt;pwd&gt;\") // ), // \"etljob\", // Some(\"/opt/docker/bin/load-data\"), // Some(\"Never\") // ) case class EtlJob5Props ( ratings_input_path: List[String] = List(\"\"), job_schedule: String = \"0 0 5,6 ? * *\", ratings_output_table: String = \"\", job_deploy_mode: Executor = kubernetes ) In above case class we define job_deploy_mode as kubernetes which tells library on how this particular jobs will gets executed."
    } ,    
    {
      "title": "Local Executor",
      "url": "/etlflow/site/docs/local.html",
      "content": "Local Executor This default mode of job execution. When we provide the job execution as local then job will gets submitted on the local environment Parameters args [EtlJobArgs] - Job Details. transactor [HikariTransactor[Task]] - Database connection details. Example 1 Below is the example to tell library about job should get executed on dataproc cluster. import etlflow.schema.Executor case class EtlJob5Props ( ratings_input_path: List[String] = List(\"\"), job_schedule: String = \"0 0 5,6 ? * *\", ratings_output_table: String = \"\", job_deploy_mode: Executor = Executor.LOCAL ) In above case class we define job_deploy_mode as local which tells library on how this particular jobs will gets executed."
    } ,    
    {
      "title": "Local Sub-Process Executor",
      "url": "/etlflow/site/docs/local_subprocess.html",
      "content": "Local Sub-Process Executor When we provides the job execution as local sub process then job will gets submit on another jvm as local process Parameters args [EtlJobArgs] - Job Details. transactor [HikariTransactor[Task]] - Database connection details. Example 1 Below is the example to tell library about job should get executed on local subprocess mode. import etlflow.schema.{Executor} import etlflow.schema.Executor.LOCAL_SUBPROCESS val local_subprocess = LOCAL_SUBPROCESS(\"examples/target/docker/stage/opt/docker/bin/load-data\",heap_min_memory = \"-Xms100m\",heap_max_memory = \"-Xms100m\") // local_subprocess: LOCAL_SUBPROCESS = LOCAL_SUBPROCESS( // \"examples/target/docker/stage/opt/docker/bin/load-data\", // \"-Xms100m\", // \"-Xms100m\" // ) case class EtlJob5Props ( ratings_input_path: List[String] = List(\"\"), job_schedule: String = \"0 0 5,6 ? * *\", ratings_output_table: String = \"\", job_deploy_mode: Executor = local_subprocess ) In above case class we define job_deploy_mode as local subprocess which tells library on how this particular jobs will gets executed."
    } ,    
    {
      "title": "Server Installation",
      "url": "/etlflow/site/docs/localdocker.html",
      "content": "Using Etlflow Server library on local environment with Docker Compose Prerequisite Your system should have Docker installed. You should have GCP service account access with appropriate permissions for GCS, BigQuery, Submitting jobs on Dataproc etc. STEP 1) Building docker image for examples project. Update the file examples/src/main/resources/application.conf with below content : db-log = { url = &lt;log_db_url&gt;, user = &lt;log_db_user&gt;, password = &lt;log_db_pwd&gt;, driver = \"org.postgresql.Driver\" } slack = { url = &lt;slack-url&gt;, env = &lt;running-env&gt; } Copy GCP service account json file at location examples/src/main/conf with name cred.json To build docker image we have to run below commands. &gt; sbt &gt; project examples &gt; docker:publishLocal STEP 2) Now we have successfully built an image. Let’s run this image with help of docker-compose file. Below Docker compose file file contains the two services : postgres Service: Required to log information in postgres db. webserver Service: Scheduler and webserver for running etljobs based on cron. version: '3.4' services: postgres: image: 'postgres:9.6' deploy: resources: limits: memory: 512M environment: - POSTGRES_USER=etlflow - POSTGRES_PASSWORD=etlflow - POSTGRES_DB=etlflow ports: - '5432:5432' webserver: image: 'etlflow:0.7.18' deploy: resources: limits: memory: 2G restart: always depends_on: - postgres environment: GOOGLE_APPLICATION_CREDENTIALS: /opt/docker/conf/cred.json LOG_DB_URL: 'jdbc:postgresql://postgres:5432/etlflow' LOG_DB_USER: etlflow LOG_DB_PWD: etlflow LOG_DB_DRIVER: org.postgresql.Driver ports: - '8080:8080' Use below commands to run/stop containers defined by docker-compose file: docker-compose --compatibility config (To check config) docker-compose --compatibility up -d (To start the service in background) docker-compose --compatibility down (To stop the service) docker logs etlflow_webserver_1 (To check the deployed container logs) Note Below are the settings for docker configuration in examples.sbt: packageName in Docker := \"etlflow\", mainClass in Compile := Some(\"examples.RunCustomServer\"), dockerBaseImage := \"openjdk:jre\", dockerExposedPorts ++= Seq(8080), mappings.in(Universal) += (sourceDirectory.value / \"main\" / \"conf\" / \"loaddata.properties\", \"conf/loaddata.properties\"), mappings.in(Universal) += (sourceDirectory.value / \"main\" / \"conf\" / \"cred.json\", \"conf/cred.json\") Here you can see we are copying the loaddata.properties file and cred.json file inside docker image. Setting up a docker environment"
    } ,    
    {
      "title": "Parallel Step Execution",
      "url": "/etlflow/site/docs/parallel.html",
      "content": "Http Step This page shows how to execute more than one steps in parallel Example 1 Below is the sample Http Step example. import etlflow.etlsteps._ import etlflow.etlsteps.ParallelETLStep import etlflow.etljobs.SequentialEtlJob import etlflow.EtlJobProps import etlflow.EtlStepList import etlflow.utils.HttpMethod case class EtlJob1Props ( ratings_output_table_name: String = \"ratings\", ) extends EtlJobProps case class Job1SparkS3andGCSandBQSteps(job_properties: EtlJob1Props) extends SequentialEtlJob[EtlJob1Props] { val postStep1 = HttpRequestStep[Unit]( name = \"HttpPostJson\", url = \"https://httpbin.org/post\", method = HttpMethod.POST, params = Left(\"\"\"{\"key\":\"value\"}\"\"\"), headers = Map(\"X-Auth-Token\"-&gt;\"abcd.xxx.123\"), log = true, ) val postStep2 = HttpRequestStep[Unit]( name = \"HttpPostForm\", url = \"https://httpbin.org/post?signup=yes\", method = HttpMethod.POST, params = Right(Map(\"name\" -&gt; \"John\", \"surname\" -&gt; \"doe\")), log = true, ) val parstep = ParallelETLStep(\"ParallelStep\")(postStep1,postStep2) val etlStepList: List[EtlStep[Unit, Unit]] = EtlStepList(parstep) }"
    } ,      
    {
      "title": "Redis Query",
      "url": "/etlflow/site/docs/redisquery.html",
      "content": "Redis Query Step This page shows use of redis query step available in this library Parameters name [String] - Description of the Step. command [String] - Action to perform(RedisCmd.SET/RedisCmd.DELETE/RedisCmd.FLUSHALL). credentials [REDIS] - Redis credentials Example 1 import etlflow.EtlStepList import etlflow.etlsteps.{EtlStep, RedisStep} import etlflow.schema.Credential.REDIS import etlflow.etlsteps.RedisStep.RedisCmd val redis_config: REDIS = REDIS(\"localhost\") // redis_config: REDIS = REDIS(\"localhost\", None, 6379) val step1 = RedisStep( name = \"Set redis key and value\", command = RedisCmd.SET(Map(\"key1\" -&gt; \"value1\",\"key2\" -&gt; \"value3\",\"key3\" -&gt; \"value3\")), credentials = redis_config ) // step1: RedisStep = etlflow.etlsteps.RedisStep@5856cbd6 val step2 = RedisStep( name = \"delete the keys from redis\", command = RedisCmd.DELETE(List(\"*key1*\")), credentials = redis_config ) // step2: RedisStep = etlflow.etlsteps.RedisStep@4697b15a val step3 = RedisStep( name = \"flushall the keys from redis\", command = RedisCmd.FLUSHALL, credentials = redis_config ) // step3: RedisStep = etlflow.etlsteps.RedisStep@cacbe26"
    } ,    
    {
      "title": "Remote Steps",
      "url": "/etlflow/site/docs/remote.html",
      "content": "Remote Steps This page shows different Remote Steps available in this library EtlFlowJobStep : We can use below step when we want to submit child job from parent job. Child Job import etlflow.EtlStepList import etlflow.etljobs.SequentialEtlJob import etlflow.etlsteps.{EtlStep, GenericETLStep} import etlflow.EtlJobProps case class EtlJob4Props ( ratings_input_path: String = \"input_path\", ) extends EtlJobProps case class HelloWorldJob(job_properties: EtlJob4Props) extends SequentialEtlJob[EtlJob4Props] { def processData(ip: Unit): Unit = { logger.info(\"Hello World\") } val step1 = GenericETLStep( name = \"ProcessData\", transform_function = processData, ) override def etlStepList: List[EtlStep[Unit, Unit]] = EtlStepList(step1) } Parent Job import etlflow.etlsteps.EtlFlowJobStep val step = EtlFlowJobStep[EtlJob4Props]( name = \"Test\", job = HelloWorldJob(EtlJob4Props()), ) // step: EtlFlowJobStep[EtlJob4Props] = etlflow.etlsteps.EtlFlowJobStep@30fede62"
    } ,    
    {
      "title": "S3 Steps",
      "url": "/etlflow/site/docs/s3.html",
      "content": "AWS S3 Steps S3PutStep This step uploads file from local filesystem to S3 bucket in specified key. Parameters bucket [String] - Name of the GCS bucket. prefix [String] - Prefix of the object. key [String] - The key(folder) inside S3 bucket. file [String] - This is the full local path of the file which needs to be uploaded. region [Duration] - S3 region where bucket is present. import etlflow.etlsteps._ import software.amazon.awssdk.regions.Region val step: S3PutStep = S3PutStep( name = \"LoadRatingS3\", bucket = \"s3_bucket\", key = \"temp/ratings.parquet\", file = \"local_file_path\", region = Region.AP_SOUTH_1 ) // step: S3PutStep = etlflow.etlsteps.S3PutStep@1f42c075 S3DeleteStep [Documentation coming soon]"
    } ,    
    {
      "title": "S3 Sensor",
      "url": "/etlflow/site/docs/s3sensor.html",
      "content": "AWS S3 Sensor Step This step runs S3 lookup for specified key in bucket repeatedly until it is found or fail after defined number of retries. Parameters bucket [String] - Name of the GCS bucket. prefix [String] - Prefix of the object. key [String] - The key being waited on. It is relative path from root level after prefix. retry [Int] - Number of retries before failing the step. spaced [Duration] - Each retry is done after waiting of duration specified in this field. region [Region] - S3 region where bucket is present. Example 1 Below example waits for a file temp/ratings.parquet to be present in a S3 bucket named s3_bucket. import etlflow.etlsteps._ import software.amazon.awssdk.regions.Region import scala.concurrent.duration._ val step = S3SensorStep( name = \"S3KeySensor\", bucket = \"s3_bucket\", prefix = \"temp\", key = \"ratings.parquet\", retry = 10, spaced = 5.second, region = Region.AP_SOUTH_1 ) // step: S3SensorStep = etlflow.etlsteps.S3SensorStep@50375825"
    } ,      
    {
      "title": "Send Mail",
      "url": "/etlflow/site/docs/sendmail.html",
      "content": "Send Mail Step This page shows Send Mail Step available in this library Parameters name [String] - Description of the Step. body [String] - Email body. subject [String] - Subject of the email. recipient_list [List(String)] - Recipient list. credentials [SMTP] - smtp credentials Example 1 Below is the sample example for send mail step import etlflow.etlsteps._ import etlflow.schema.Credential.SMTP import java.time.LocalDateTime import java.time.format.DateTimeFormatter val emailBody: String = { val exec_time = DateTimeFormatter.ofPattern(\"yyyy-MM-dd_HH:mm\").format(LocalDateTime.now) s\"\"\" | SMTP Email Test | Time of Execution: $exec_time |\"\"\".stripMargin } // emailBody: String = \"\"\" // SMTP Email Test // Time of Execution: 2021-07-21_08:03 // \"\"\" val step = SendMailStep( name = \"SendSMTPEmail\", body = emailBody, subject = \"EtlFlow Ran Successfully\", recipient_list = List(\"abc@&lt;domain&gt;.com\"), credentials = SMTP(\"PORT\", \"HOST\", \"USER\", \"PWD\") ) // step: SendMailStep = SendMailStep( // \"SendSMTPEmail\", // \"\"\" // SMTP Email Test // Time of Execution: 2021-07-21_08:03 // \"\"\", // \"EtlFlow Ran Successfully\", // None, // List(\"abc@&lt;domain&gt;.com\"), // SMTP(\"PORT\", \"HOST\", \"USER\", \"PWD\", \"smtp\", \"true\", \"true\") // )"
    } ,    
    {
      "title": "Sensors",
      "url": "/etlflow/site/docs/sensors.html",
      "content": "Sensors As Step can perform any process that can be executed in Scala. Similarly, Sensors can check the state of any process or data structure. They can be used to pause the execution of dependent steps until some criterion has been met. Basically they are steps with additional retry capabilities. Following is the list of sensors available in this library: GCSSensorStep S3SensorStep JDBCSensorStep"
    } ,    
    {
      "title": "Etlflow Server",
      "url": "/etlflow/site/docs/server.html",
      "content": "Quickstart (Etlflow Server) Etlflow Server is one of the module of etlflow library which can be used for scheduling, triggering, monitoring the jobs. Unique feature of this is that we can run the same etl job in different modes like Local (Inside same jvm), LocalSubProcess (In another jvm), GCP DataProc, Kubernetes etc. Job mode can be set using job properties. Etlflow Server contains sub-components like: Scheduler: Webserver is the component which provides funnctionality to run jobs based on some cron schedule, this internally uses Cron4s. Cron expression can be specified in job properties, see below example EtlJob1Props for more details. All the scheduled jobs will run Asynchronously in non blocking way which allows us to schedule 1000’s of job without any issues. Webserver: Webserver is the component which provides us with UI, GraphQL API and Rest API. It uses Http4s as backend. We can do job submission, check the job and step execution, monitor server health etc through the UI. Executor: Executor is responsible for actually running the job. It contains various different executors such as: Local Local-Sub-Process DataProc Kubernetes Architecture Diagram : In above diagram we can see docker container is connected to database (postgres) which is getting used for storing all the etl jobs related information in database tables. STEP 1) Define build.sbt: To use etlflow server library in project add below setting in build.sbt file lazy val root = (project in file(\".\")) .settings( scalaVersion := \"2.12.13\", libraryDependencies ++= List(\"com.github.tharwaninitin\" %% \"etlflow-core\" % \"x.x.x\")) STEP 2) Define job properties EtlJobProps.scala: Here we can have any kind of logic for creating static or dynamic input parameters for job. For e.g. intermediate path can be dynamically generated for every run based on current date. import etlflow.EtlJobProps import java.text.SimpleDateFormat import java.time.LocalDate lazy val canonical_path = new java.io.File(\".\").getCanonicalPath lazy val input_file_path = s\"$canonical_path/modules/core/src/test/resources/input/movies/ratings_parquet/ratings.parquet\" val date_prefix = LocalDate.now.toString.replace(\"-\",\"\") // date_prefix: String = \"20210721\" case class EtlJob1Props ( ratings_input_path: String = input_file_path, ratings_intermediate_bucket: String = sys.env(\"GCS_BUCKET\"), ratings_intermediate_file_key: String = s\"temp/$date_prefix/ratings.parquet\", ratings_output_dataset: String = \"test\", ratings_output_table_name: String = \"ratings\", ) extends EtlJobProps STEP 3) Define job EtlJob1.scala: Below is the example of GenericEtlJob which has two steps which can execute in any order defined by composing ZIO effects. import com.google.cloud.bigquery.JobInfo import etlflow.etljobs.GenericEtlJob import etlflow.etlsteps.{BQLoadStep, GCSPutStep} import zio.Task import etlflow.spark.IOType import etlflow.gcp.BQInputType case class RatingOutput(user_id: Int, movie_id: Int, rating : Double, timestamp: Long, date: java.sql.Date) case class EtlJob1(job_properties: EtlJob1Props) extends GenericEtlJob[EtlJob1Props] { val step1 = GCSPutStep( name = \"LoadRatingGCS\", bucket = job_properties.ratings_intermediate_bucket, key = job_properties.ratings_intermediate_file_key, file = job_properties.ratings_input_path ) val step2 = BQLoadStep( name = \"LoadRatingBQ\", input_location = Left(s\"gs://${job_properties.ratings_intermediate_bucket}/${job_properties.ratings_intermediate_file_key}\"), input_type = BQInputType.PARQUET, output_dataset = job_properties.ratings_output_dataset, output_table = job_properties.ratings_output_table_name, output_create_disposition = JobInfo.CreateDisposition.CREATE_IF_NEEDED ) val job = for { _ &lt;- step1.execute() _ &lt;- step2.execute() } yield () } STEP 4) Define job and properties mapping using EtlJobPropsMapping MyEtlJobPropsMapping.scala: Below is the example of defining EtlJobPropsMapping where we can define the mapping between job and its properties. import etlflow.{EtlJobPropsMapping, EtlJobProps} import etlflow.etljobs.EtlJob sealed trait MyEtlJobPropsMapping[EJP &lt;: EtlJobProps, EJ &lt;: EtlJob[EJP]] extends EtlJobPropsMapping[EJP,EJ] object MyEtlJobPropsMapping { case object Job1 extends MyEtlJobPropsMapping[EtlJob1Props,EtlJob1] { override def getActualProperties(job_properties: Map[String, String]): EtlJob1Props = EtlJob1Props() } } STEP 5) Define Main Runnable Server App RunServer.scala: import etlflow.{ServerApp, EtlJobProps} object RunServer extends ServerApp[MyEtlJobPropsMapping[EtlJobProps,EtlJob[EtlJobProps]]] STEP 6) Running Server: To be able to use this server library, first you need postgres instance up and running, then create new database in pg (for e.g. etlflow), then set below environment variables. export LOG_DB_URL=jdbc:postgresql://localhost:5432/etlflow export LOG_DB_USER=&lt;...&gt; export LOG_DB_PWD=&lt;..&gt; export LOG_DB_DRIVER=org.postgresql.Driver Now to create database tables used in this library run below commands from repo root folder: sbt runMain RunServer run_db_migration Now to run sample job use below command: sbt runMain RunServer"
    } ,    
    {
      "title": "Spark",
      "url": "/etlflow/site/docs/spark.html",
      "content": "Apache Spark Steps This page shows different Spark Steps available in this library Below are the Input/Output formats supported by this step: Input Formats =&gt; CSV, JSON, ORC, PARQUET, JDBC, BQ Output Formats =&gt; CSV, JSON, ORC, PARQUET, JDBC SparkReadWriteStep We can use below step when : We want load the data from above mentioned source format into destination format. When there is no need of transformation function. Create spark session import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory import ch.qos.logback.classic.{Level, Logger} LoggerFactory.getLogger(\"org\").asInstanceOf[Logger].setLevel(Level.WARN) implicit lazy val spark: SparkSession = SparkSession.builder().master(\"local[*]\").getOrCreate() import etlflow.etlsteps._ import etlflow.utils._ import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SaveMode import org.apache.spark.sql.{Encoders, Dataset} import org.apache.spark.sql.types.DateType import org.apache.spark.sql.functions._ import etlflow.spark.IOType import etlflow.gcp.BQInputType import etlflow.schema.Credential.JDBC case class Rating(user_id: Int, movie_id: Int, rating: Double, timestamp: Long) lazy val step1 = SparkReadWriteStep[Rating]( name = \"LoadRatingsParquetToJdbc\", input_location = Seq(\"gs://path/to/input/*\"), input_type = IOType.PARQUET, output_type = IOType.RDB(JDBC(\"jdbc_url\", \"jdbc_user\", \"jdbc_pwd\", \"jdbc_driver\")), output_location = \"ratings\", output_save_mode = SaveMode.Overwrite ) SparkReadTransformWriteStep We can use below step when : We want load the data from above mentioned source format into destination format. When there is need of transformation function. case class RatingOutputCsv(user_id: Int, movie_id: Int, rating : Double, timestamp: Long, date: java.sql.Date) def enrichRatingCsvData(spark: SparkSession, in: Dataset[Rating]): Dataset[RatingOutputCsv] = { val mapping = Encoders.product[RatingOutputCsv] val ratings_df = in .withColumnRenamed(\"user_id\",\"User Id\") .withColumnRenamed(\"movie_id\",\"Movie Id\") .withColumnRenamed(\"rating\",\"Ratings\") .withColumn(\"date\", from_unixtime(col(\"timestamp\"), \"yyyy-MM-dd\").cast(DateType)) .withColumnRenamed(\"date\",\"Movie Date\") ratings_df.as[RatingOutputCsv](mapping) } lazy val step3 = SparkReadTransformWriteStep[Rating, RatingOutputCsv]( name = \"LoadRatingsCsvToCsv\", input_location = Seq(\"gs://path/to/input/\"), input_type = IOType.CSV(), transform_function = enrichRatingCsvData, output_type = IOType.CSV(), output_location = \"gs://path/to/output/\", output_save_mode = SaveMode.Overwrite, output_filename = Some(\"ratings.csv\") ) Whenever we want to write the data using above mentioned steps into partitioned format then dont specify the output_filename as a parameter"
    } ,    
    {
      "title": "Etlflow Spark",
      "url": "/etlflow/site/docs/spark_components.html",
      "content": "Etlflow Spark To use etlflow Spark library in project add below setting in build.sbt file : lazy val etlflowSpark = ProjectRef(uri(\"git://github.com/tharwaninitin/etlflow.git#x.x.x\"), \"spark\") lazy val docs = (project in file(\"modules/examples\")) .dependsOn(etlflowSpark) Etlflow Spark library contains modules like : AWS : This module can get used when we want to read/write data to and from aws buckets. GCP : This module can get used when we want to read/write data to and from gcp buckets. STEP 1) Building an assembly jar for etlflow Spark To build an assembly jar for an etlflow spark library we need to perform some steps. Clone this git repo and go inside repo root folder and perform below steps: &gt; sbt &gt; project spark &gt; assembly Inside root folder run ‘sbt’ command Inside sbt, Run ‘project spark’ to set the current project as etlflow-spark Once project gets set then run ‘assembly’ to build an assembly jar. STEP 2) Copy the etlflow spark assembly jar to gcs bucket using below command gsutil cp modules/spark/target/scala-2.12/etlflow-spark-assembly-x.x.x.jar gs://&lt;BUCKET-NAME&gt;/jars/examples Replace x.x.x with the latest version Example : Below is the step of spark library using which we can use the sparksession and transform the data : Create spark session import org.apache.spark.sql.SparkSession import org.slf4j.LoggerFactory import ch.qos.logback.classic.{Level, Logger} LoggerFactory.getLogger(\"org\").asInstanceOf[Logger].setLevel(Level.WARN) implicit lazy val spark: SparkSession = SparkSession.builder().master(\"local[*]\").getOrCreate() import etlflow.etlsteps._ import etlflow.utils._ import org.apache.spark.sql.SparkSession import org.apache.spark.sql.SaveMode import org.apache.spark.sql.{Encoders, Dataset} import org.apache.spark.sql.types.DateType import org.apache.spark.sql.functions._ import etlflow.etljobs.GenericEtlJob import etlflow.EtlJobProps import etlflow.spark.IOType import etlflow.gcp.BQInputType import etlflow.schema.Credential.JDBC case class Rating(user_id: Int, movie_id: Int, rating: Double, timestamp: Long) extends EtlJobProps case class EtlJob1(job_properties: Rating) extends GenericEtlJob[Rating] { lazy val step1 = SparkReadWriteStep[Rating]( name = \"LoadRatingsParquetToJdbc\", input_location = Seq(\"gs://path/to/input/*\"), input_type = IOType.PARQUET, output_type = IOType.RDB(JDBC(\"jdbc_url\", \"jdbc_user\", \"jdbc_pwd\", \"jdbc_driver\")), output_location = \"ratings\", output_save_mode = SaveMode.Overwrite ) val job = for { _ &lt;- step1.execute() } yield () }"
    } ,    
    {
      "title": "EtlSteps",
      "url": "/etlflow/site/docs/steps.html",
      "content": "Steps Step is fundamental unit for describing task in EtlFlow. Steps are atomic, meaning they can run on their own or together inside Job. Following is the list of steps available in this library: SparkReadWriteStep SparkReadTransformWriteStep BQLoadStep BQQueryStep S3PutStep GCSPutStep DBQueryStep DBReadStep SendMailStep RedisQueryStep HttpStep HttpResponseStep DataprocHiveStep DataprocJobStep"
    } ,    
    {
      "title": "Project Structure",
      "url": "/etlflow/site/docs/structure.html",
      "content": "Project Structure EtlFlow project contains following modules: modules/core: This module contains core library which defines Scala internal dsl that assists with writing ETL Job which can be composed as multiple ETL steps in a concise manner which facilitates easier Testing and reasoning about the entire job. This module also contains many test jobs which contains multiple steps. This core library also contains tests and all jobs uses EtlJob API. To run all test successfully some properties needs to be set in application.conf or set these properties as ENVIRONMENT variables. export GOOGLE_APPLICATION_CREDENTIALS=&lt;...&gt; # this should be full path to Service Account Key Json which should have GCS and Biguery Read/Write access Now run tests using below sbt command sbt \"project etlflow\" test modules/core/src/main/scala/etlflow/etlsteps: This package contains all type of ETL Steps that can be created with this library. modules/examples: This module provides examples of different types of ETL Jobs which can be created with this library."
    } ,    
    {
      "title": "Usage Guides",
      "url": "/etlflow/site/docs/usage.html",
      "content": "Usage Guides Below is the list of various ways in which this library can be used: GCP Dataproc : Using Etlflow core library in your ETL projects on GCP Dataproc Local Docker : Using Etlflow server library on local environment with Docker Compose Kubernetes Engine : Using Etlflow server library on Google Kubernetes Engine."
    }    
  ];

  idx = lunr(function () {
    this.ref("title");
    this.field("content");

    docs.forEach(function (doc) {
      this.add(doc);
    }, this);
  });

  docs.forEach(function (doc) {
    docMap.set(doc.title, doc.url);
  });
}

// The onkeypress handler for search functionality
function searchOnKeyDown(e) {
  const keyCode = e.keyCode;
  const parent = e.target.parentElement;
  const isSearchBar = e.target.id === "search-bar";
  const isSearchResult = parent ? parent.id.startsWith("result-") : false;
  const isSearchBarOrResult = isSearchBar || isSearchResult;

  if (keyCode === 40 && isSearchBarOrResult) {
    // On 'down', try to navigate down the search results
    e.preventDefault();
    e.stopPropagation();
    selectDown(e);
  } else if (keyCode === 38 && isSearchBarOrResult) {
    // On 'up', try to navigate up the search results
    e.preventDefault();
    e.stopPropagation();
    selectUp(e);
  } else if (keyCode === 27 && isSearchBarOrResult) {
    // On 'ESC', close the search dropdown
    e.preventDefault();
    e.stopPropagation();
    closeDropdownSearch(e);
  }
}

// Search is only done on key-up so that the search terms are properly propagated
function searchOnKeyUp(e) {
  // Filter out up, down, esc keys
  const keyCode = e.keyCode;
  const cannotBe = [40, 38, 27];
  const isSearchBar = e.target.id === "search-bar";
  const keyIsNotWrong = !cannotBe.includes(keyCode);
  if (isSearchBar && keyIsNotWrong) {
    // Try to run a search
    runSearch(e);
  }
}

// Move the cursor up the search list
function selectUp(e) {
  if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index) && (index > 0)) {
      const nextIndexStr = "result-" + (index - 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Move the cursor down the search list
function selectDown(e) {
  if (e.target.id === "search-bar") {
    const firstResult = document.querySelector("li[id$='result-0']");
    if (firstResult) {
      firstResult.firstChild.focus();
    }
  } else if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index)) {
      const nextIndexStr = "result-" + (index + 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Search for whatever the user has typed so far
function runSearch(e) {
  if (e.target.value === "") {
    // On empty string, remove all search results
    // Otherwise this may show all results as everything is a "match"
    applySearchResults([]);
  } else {
    const tokens = e.target.value.split(" ");
    const moddedTokens = tokens.map(function (token) {
      // "*" + token + "*"
      return token;
    })
    const searchTerm = moddedTokens.join(" ");
    const searchResults = idx.search(searchTerm);
    const mapResults = searchResults.map(function (result) {
      const resultUrl = docMap.get(result.ref);
      return { name: result.ref, url: resultUrl };
    })

    applySearchResults(mapResults);
  }

}

// After a search, modify the search dropdown to contain the search results
function applySearchResults(results) {
  const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
  if (dropdown) {
    //Remove each child
    while (dropdown.firstChild) {
      dropdown.removeChild(dropdown.firstChild);
    }

    //Add each result as an element in the list
    results.forEach(function (result, i) {
      const elem = document.createElement("li");
      elem.setAttribute("class", "dropdown-item");
      elem.setAttribute("id", "result-" + i);

      const elemLink = document.createElement("a");
      elemLink.setAttribute("title", result.name);
      elemLink.setAttribute("href", result.url);
      elemLink.setAttribute("class", "dropdown-item-link");

      const elemLinkText = document.createElement("span");
      elemLinkText.setAttribute("class", "dropdown-item-link-text");
      elemLinkText.innerHTML = result.name;

      elemLink.appendChild(elemLinkText);
      elem.appendChild(elemLink);
      dropdown.appendChild(elem);
    });
  }
}

// Close the dropdown if the user clicks (only) outside of it
function closeDropdownSearch(e) {
  // Check if where we're clicking is the search dropdown
  if (e.target.id !== "search-bar") {
    const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
    if (dropdown) {
      dropdown.classList.remove("show");
      document.documentElement.removeEventListener("click", closeDropdownSearch);
    }
  }
}
